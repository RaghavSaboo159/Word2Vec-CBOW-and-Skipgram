# -*- coding: utf-8 -*-
"""skipm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oti5xw-zr2Yqh4iwm3bmOm_5qdc9nIZx
"""

!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz

!gzip -d reviews_Electronics_5.json.gz

!ls

import numpy as np
import re
from collections import defaultdict
import json
import nltk
import operator
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize,RegexpTokenizer
from tqdm import tqdm
from scipy.special import expit as sigmoid

# nltk.download('punkt')
# nltk.download('stopwords')

def dataset_prep(f_open):
    # stop_words = set(stopwords.words('english')) 
    data_Set=[]
    # print(len(data_Set))

    for i in f_open:
        data=json.loads(i)
        data_Set.append(data['reviewText'])
        if(len(data_Set)>=50000):
            break

    # print(data_Set)
    # exit(0)
    # print(len(data_Set))
    token=RegexpTokenizer(r"\w+")
    f_data=[]
    for i in data_Set:
        output=re.sub(r'\d+','',i)
        new=token.tokenize(output)
        datan = [sent.lower() for sent in new]
        f_data.append(datan)

        # print(new)
    # print(f_data)
    # exit(0)
    return f_data

class word2vec():
    def __init__ (self):
        self.n = settings['n']
        self.eta = settings['learning_rate']
        self.epochs = settings['epochs']
        self.window = settings['window_size']
        self.batch = settings['batch']
       	self.word2Ind = {}
        self.Ind2word = {}
        self.negative=settings['negative']

        self.f=0

        pass
    
    
    # GENERATE TRAINING DATA
    def generate_indices(self, settings, corpus):
        self.word_counts=defaultdict(int)
        self.u_word_counts=defaultdict(int)
        # self.word_counts = defaultdict(int)
        for row in corpus:
            for word in row:
                self.word_counts[word] += 1
        for key in self.word_counts:
            if self.word_counts[key] > 3:
                self.u_word_counts[key] =self.word_counts[key]

        # print(self.u_word_counts)
        self.word_counts=self.u_word_counts
        # exit(0)
        self.v_count = len(self.word_counts.keys())
        # print(self.v_count)
        # GENERATE LOOKUP DICTIONARIES
        self.words_list = sorted(list(self.word_counts.keys()),reverse=False)
        self.word2Ind = dict((word, i) for i, word in enumerate(self.words_list))
        self.Ind2word = dict((i, word) for i, word in enumerate(self.words_list))
        self.w1=np.random.uniform(0,1,(self.v_count,self.n))
        self.w2=np.random.uniform(0,1,(self.n,self.v_count))
        self.wordfreq=np.zeros(self.v_count)
        idx=0;
        for key in self.word_counts:
            self.wordfreq[idx]=self.word_counts[key]
            idx+=1
        # print(self.wordfreq)
        self.wordprob=self.wordfreq**0.75
        self.wordprob/=self.wordprob.sum()
        print(self.wordprob)
        # for sent in corpus:
        #     for token in sent:
	       #      words.add(token)
        # words = sorted(list(words))
        # self.v_count = len(words)
        # print(self.v_count)
	    # idx = 0
	    # for k in words:
	    #     self.word2Ind[k] = idx
	    #     self.Ind2word[idx] = k
	    #     idx += 
        # self.w1 = np.random.uniform(0, 0.1, (self.v_count, self.n))
        # self.w2=np.random.uniform(0,1,(self.n,self.v_count))
        # exit(0)
        # self.w2 = np.random.uniform(0, 1, (self.n, self.v_count))

        # print()
        # self.words_list = sorted(list(self.word_counts.keys()),reverse=False)
        # self.word_index = dict((word, i) for i, word in enumerate(self.words_list))
        # self.index_word = dict((i, word) for i, word in enumerate(self.words_list))
        # pass
    def generate_data(self, settings, corpus):

        training_data = []
        # CYCLE THROUGH EACH SENTENCE IN CORPUS
        for sentence in corpus:
            sent_len = len(sentence)

            # CYCLE THROUGH EACH WORD IN SENTENCE
            for i, word in enumerate(sentence):
                
                #w_target  = sentence[i]
                # print(i,word)
                # exit(0)
                # if self.word_counts[word]>5 :
                w_target = self.word2onehot(sentence[i])

                # CYCLE THROUGH CONTEXT WINDOW
                w_context = []
                for j in range(i-self.window, i+self.window+1):
                    if j!=i and j<=sent_len-1 and j>=0:
                        w_context.append(self.word2onehot(sentence[j]))
                training_data.append([w_context,w_target])
        # print(training_data)
        # exit(0)
        return np.array(training_data)
    def get_context(self,index, data):
        ws = self.window
        context_words = []

        c_indx = index - 1 
        while(ws != 0):
            if(c_indx >= 0):
                # if(data[c_indx] in word2index and np.random.random() < (1 - prob_subsampling[word2index[data[c_indx]]])):
                if(data[c_indx] in self.word2Ind):
                    if(self.word2Ind[data[c_indx]] not in context_words):
                        context_words.append(self.word2Ind[data[c_indx]])
                        ws -= 1
            else:
                break

            c_indx -= 1 

        
        ws = self.window
        c_indx = index + 1
        while(ws != 0):
            if(c_indx < len(data)):
                if(data[c_indx] in self.word2Ind):
                # if(data[c_indx] in word2index and np.random.random() < (1 - prob_subsampling[word2index[data[c_indx]]])):
                    if(self.word2Ind[data[c_indx]] not in context_words):
                        context_words.append(self.word2Ind[data[c_indx]])
                        ws -= 1
            else:
                break

            c_indx += 1 


        return np.array(context_words)


    def get_vectors(self,data):
	    samples = 0
	    C=2
	    while True:
	        # epoch += 1
	        # print(f"new epoch {epoch}, samples are {samples}")
	        for sent in data:
	            # prob = np.random.uniform(0, 1)
	            # if prob > 0.4:
	                # continue
	            len_sent = len(sent)
	            for i in range(len_sent):
                    # x=np.zeros(self.v_count)
	                y = np.zeros(self.v_count)
	                x = np.zeros(self.v_count)
	                y[self.word2Ind[sent[i]]] = 1
	                mean_cnt = 0
	                for j in range(max(0, i - C), min(i + C + 1, len_sent)):
	                    if j != i:
	                        x[self.word2Ind[sent[j]]] += 1
	                        mean_cnt += 1

	                if mean_cnt > 0:
	                    x /= mean_cnt
	                samples += 1
	                yield x, y 	

    # SOFTMAX ACTIVATION FUNCTION
    def softmax(self, x):
    	# e = np.exp(x - np.max(x))
    	# if e.ndim == 1:
	    # 	return e / np.sum(e, axis=0)
	    # else: # dim = 2
	    # 	return e / np.sthatum(e, axis=1, keepdims=True)
    	# print(x)
	    e_z = np.exp(x-np.max(x))
	    if(e_z.ndim==1):
	    	return e_z/np.sum(e_z, axis=0)
	    else:
	    	return e_z/np.sum(e_z,axis=1,keepdims=True)
	    # exit(0)
	    # return yhat


    # CONVERT WORD TO ONE HOT ENCODING
    # def word2onehot(self, word):
    #     word_vec = [0 for i in range(0, self.v_count)]
    #     word_index = self.word_index[word]
    #     word_vec[word_index] = 1
    #     return word_vec


    # FORWARD PASS
    def forward_pass(self, main_word,context_words,negsamples):
        targets = list()
        # print(context_words)
        for context_word in context_words:
            targets.append(context_word)

        for negsample in negsamples:
            targets.append(negsample)

        targets = np.array(targets)
        # print(targets)
    #
        #forward_propagation

        h = self.w1[main_word]
        # print(h)
        # print(W_output[:,targets]   )
        prediction = np.dot(h,self.w2[:,targets])
        # print(prediction)
        # exit(0)

        prediction = sigmoid(prediction)
        return prediction,targets
                

    # BACKPROPAGATION
    def backprop(self,prediction,targets,main_word,context_words ):
        tj = np.zeros(len(targets))
        for i in range(len(context_words)):
            tj[i] = 1

        pred_error = prediction - tj
        del_W_input = pred_error.dot(self.w2[:,targets].T)
        del_W_output = np.outer(pred_error,self.w1[main_word])
        self.w1[main_word] = self.w1[main_word] - self.eta*del_W_input
        W_outputt = self.w2.T
        for index in range(len(targets)):
            W_outputt[targets[index]] = W_outputt[targets[index]] - self.eta*del_W_output[index]

        self.w2 = W_outputt.T  

    	# dl_dw2 = np.dot(h.T, e)/self.batch
    	# dl_dw1 = np.dot(x, np.dot(self.w2, e.T).T)/self.batch
    	# self.w1 = self.w1 - (self.eta * dl_dw1)
    	# self.w2 = self.w2 - (self.eta * dl_dw2)
    def get_batches(self,data):
    	batch_x=[]
    	batch_y=[]
    	for x,y in self.get_vectors(data):
    		while len(batch_x)<self.batch:
    			batch_x.append(x)
    			batch_y.append(y)
    		else:
    			yield np.array(batch_x).T,np.array(batch_y).T
    			batch=[]
    # TRAIN W2V model
    def train(self, training_data):
        # # CYCLE THROUGH EACH EPOCH
        # f=0
        for i in range(0,self.epochs):
            itr=0
            for x in training_data:
                print(itr)
                itr+=1
                rnd=np.random.permutation(np.arange(len(x)))
                for idx in rnd:
                    if x[idx] in self.word2Ind:
                        main=self.word2Ind[x[idx]]
                        context=self.get_context(idx,x)
                        if (len(context)==0):
                            continue
                        neg=np.random.choice(self.v_count,size=self.negative,p=self.wordprob)
                        pred,tar = self.forward_pass(main,context,neg)
                        # EI = np.subtract(y_pred, w_c.T)
                        self.backprop(pred,tar,main,context)

            self.eta*=0.66
                # self.loss += -np.sum([u[word.index(1)] for word in w_c]) + len(w_c) * np.log(np.sum(np.exp(u)))
                # self.loss += -2*np.log(len(w_c)) -np.sum([u[word.index(1)] for word in w_c]) + (len(w_c) * np.log(np.sum(np.exp(u))))
                    
                # print ('EPOCH:',i, 'LOSS:', self.loss)
        pass


    # input a word, returns a vector (if available)
    def word_vec(self, word):
        w_index = self.word2Ind[word]
        print(w_index,word)
        v_w = self.w1[w_index]
        return v_w


    # input a vector, returns nearest word(s)
    def vec_sim(self, vec, top_n):

        # CYCLE THROUGH VOCAB
        word_sim = {}
        for i in range(self.v_count):
            v_w2 = self.w1[i]
            theta_num = np.dot(vec, v_w2)
            theta_den = np.linalg.norm(vec) * np.linalg.norm(v_w2)
            theta = theta_num / theta_den

            word = self.index_word[i]
            word_sim[word] = theta

        # words_sorted = sorted(word_sim.items(), key=lambda(word, sim):sim, reverse=True)

        for word, sim in words_sorted[:top_n]:
            print(word, sim)
            
        pass

    # input word, returns top [n] most similar words
    def word_sim(self, word, top_n):
        
        w1_index = self.word2Ind[word]
        v_w1 = self.w1[w1_index]

        # CYCLE THROUGH VOCAB
        word_sim = {}
        for i in range(self.v_count):
            v_w2 = self.w1[i]
            theta_num = np.dot(v_w1, v_w2)
            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)
            theta = theta_num / theta_den

            word = self.Ind2word[i]
            word_sim[word] = theta

       
        words_sorted = sorted(word_sim.items(), key=operator.itemgetter(1), reverse=True)
        print(word_sim)
        for word, sim in words_sorted[:top_n]:
            print( word, sim) # words_sorted = sorted(word_sim.items(), key=(word, sim):sim, reverse=True)
        # print(word_sim)
        # for word, sim in words_sorted[:top_n]:
        #     print( word, sim)
            
        pass

#--- EXAMPLE RUN --------------------------------------------------------------+

settings = {}
settings['n'] = 50                   # dimension of word embeddings
settings['window_size'] = 2         # context window +/- center word
settings['epochs'] = 2           # minimum word count
settings['negative'] = 10           # number of training epochs
settings['batch'] = 128           # number of negative words to use during training
settings['learning_rate'] = 0.03
np.random.seed(0)                   # set the seed for reproducibility

# corpus = [['the','quick','brown','fox','jumped','over','the','lazy','dog']]
f=open('reviews_Electronics_5.json')

corpus=dataset_prep(f)
# INITIALIZE W2V MODEL
w2v = word2vec()

# generate training data
w2v.generate_indices(settings, corpus)
# for i in tqdm(corpus):
#     n_data=[]
#     n_data.append(i)
#     training_data=w2v.generate_data(settings,n_data)
    # print(training_data)
    
    # print(training_data.shape)
    # print(len(i))
    # exit(0)

w2v.train(corpus)
def cosine_similarity(u, v):
    dot = np.dot(u,v.T)
    norm_u = np.sqrt(np.dot(u,u))
    norm_v = np.sqrt(np.dot(v,v))
    cosine_similarity = dot / (norm_u*norm_v)
    print(cosine_similarity)

# cosine_similarity(w2v.word_vec("truck"),w2v.word_vec("highways"))
cosine_similarity(w2v.word_vec("mother"),w2v.word_vec("father"))
cosine_similarity(w2v.word_vec("mother"),w2v.word_vec("led"))
print(w2v.word_vec("mother"))
print(w2v.word_vec("father"))
w2v.word_sim("camera",10)

# print(w2v.w1)# print(training_data)

# train word2vec model
# w2v.word_sim('fox',3)

embedding = {}

embedding['word2Ind'] = w2v.word2Ind
embedding['W_output'] = w2v.w2.tolist()

x = "Tepoch.json"
out_file = open(x,'w')
json.dump(embedding, out_file)
out_file.close()